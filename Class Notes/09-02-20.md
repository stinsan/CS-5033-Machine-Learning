### Bellman Expectation Equation for V<sub>𝜋</sub>
The Bellman Expectation Equation for _V_<sub>𝜋</sub> expresses a relationship between the value of a state and the values of its successor states. 
The mathematical definition is below:

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/015.PNG)

### Bellman Expectation Equation for q<sub>𝜋</sub>
Similarly, there is a Bellman Expectation Equation for q<sub>𝜋</sub>:

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/016.PNG)

### An Example Using the Robot Recycling Problem:
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/013.PNG)

Say we follow the policy 𝜋: "Pick an action equally at random at each state."\
This means that 𝜋 (wait | low) = 𝜋 (search | low) = 𝜋 (recharge | low) = 1/3 and 𝜋 (wait | high) = 𝜋 (search | high) = 1/2.

Now we will use the Bellman Expectation Equation and evaluate the policy:

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/017.PNG)

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/018.PNG)

We want to compute V<sub>𝜋</sub> (low) and V<sub>𝜋</sub> (high). So, the Bellman Equations above are basically a 2x2 system of equations!

### The Control Problem: Optimal Value Functions
**Definition**: The optimal state-value function _V_*(_s_) is the maximum over all policies. In other words,

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/019.PNG)

**Definition**: The optimal action-value function _q_*(_s_, _a_) is the maximum over all policies. In other words,

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/020.PNG)

We solve the control problem (and also the MDP) if we compute the optimal value function for every state.

**Theorem**: For any Markov decision problem, there exists an optimal policy 𝜋* that is better that or equal to all other policies.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/021.PNG)

Q: But how do we find an optimal policy?
A: Choose the best action to take at any given state, i.e.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/022.PNG)

### Bellman Optimality Equations
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/023.PNG)
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/024.PNG)

Now, we can combine the above equations:
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/025.PNG)

### Remarks on Ballman Optimality Equations
- The equations are non-linear due to the max function.
- In general, there is no closed form solution.
- Many iterative soution methods (policy iteration, value iteration, Q-learning)

### Dynamic Programming Methods
Useful in situations where the RL agent knows the MDP. Therefore, we have an accurate model of the environment and we can proceed with planning.

** A few things about dynamic programming **:
- "Dynamic" because the sequence of events matter.
- "Programming" from a mathematical point-of-view, optimizing a function (e.g. a policy).
- Useful in solving problems with optimal substructure and overlapping problems (MDPs satisfy both properties).
