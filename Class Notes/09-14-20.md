## Policy Evaluation
Given a policy 𝜋, compute v<sub>𝜋</sub>(s) for all s ∈ S.

Method: application of Bellman expectation equations.

- Using "synchronous" backups:\
In each iteration, go over each state s ∈ S and update v<sub>k+1</sub>(s) based on v<sub>k</sub>(s') for each successor state s' from s.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/026.PNG)

### Gridworld Example:

## Policy Improvement
Use **policy iteration**, given a policy 𝜋:
1. Evaluate 𝜋.\
V<sub>𝜋</sub>(s) = E<sub>𝜋</sub> [R<sub>t+1</sub> + 𝛾 R<sub>t+2</sub> + 𝛾<sup>2</sup> R<sub>t+3</sub> ... | S<sub>t</sub> = s]
2. Improve the policy by acting greedily with respect to v<sub>𝜋</sub>.\
𝜋' = greedy(v<sub>𝜋</sub>)
3. Evaluate 𝜋' to get V<sub>𝜋'</sub>.
4. Improve the policy by acting greedily with respect to v<sub>𝜋'</sub>.\
𝜋'' = greedy(v<sub>𝜋'</sub>)
5. Evaluate 𝜋'' to get v<sub>𝜋''</sub>.

Repeat until you converge to the optimal policy 𝜋*. If the improvements stop, then the Bellman optimality equations are satisfied.
 
Using **value iteration** to compute 𝜋*:
We use an iterative application of the Bellman optimality equations using "synchronous" backups.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/027.PNG)

**Theorem**: A policy 𝜋(s, a) achieves the optimal value from state s (i.e. v<sub>𝜋</sub>(s) = v*(s)) if and only if for any state s' reachable from s, 𝜋 achieves the optimal value from state s' (i.e. v<sub>𝜋</sub>(s') = v*(s')).

## Summary
| Problem | Tool | Algorithm |
|---------|------|-----------|
| Prediction | Bellman Expectation Equations | Iterative Policy Evaluation |
| Control | Bellman Expectation Equations + Greedy Policy Improvement | Policy Iteration |
| Control | Bellman Optimality Equation | Value Iteration |
 
