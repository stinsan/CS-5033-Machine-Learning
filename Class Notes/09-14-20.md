## Policy Evaluation
Given a policy ğœ‹, compute v<sub>ğœ‹</sub>(s) for all s âˆˆ S.

Method: application of Bellman expectation equations.

- Using "synchronous" backups:\
In each iteration, go over each state s âˆˆ S and update v<sub>k+1</sub>(s) based on v<sub>k</sub>(s') for each successor state s' from s.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/026.PNG)

### Gridworld Example:

## Policy Improvement
Use **policy iteration**, given a policy ğœ‹:
1. Evaluate ğœ‹.\
V<sub>ğœ‹</sub>(s) = E<sub>ğœ‹</sub> [R<sub>t+1</sub> + ğ›¾ R<sub>t+2</sub> + ğ›¾<sup>2</sup> R<sub>t+3</sub> ... | S<sub>t</sub> = s]
2. Improve the policy by acting greedily with respect to v<sub>ğœ‹</sub>.\
ğœ‹' = greedy(v<sub>ğœ‹</sub>)
3. Evaluate ğœ‹' to get V<sub>ğœ‹'</sub>.
4. Improve the policy by acting greedily with respect to v<sub>ğœ‹'</sub>.\
ğœ‹'' = greedy(v<sub>ğœ‹'</sub>)
5. Evaluate ğœ‹'' to get v<sub>ğœ‹''</sub>.

Repeat until you converge to the optimal policy ğœ‹*. If the improvements stop, then the Bellman optimality equations are satisfied.
 
Using **value iteration** to compute ğœ‹*:
We use an iterative application of the Bellman optimality equations using "synchronous" backups.
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/027.PNG)

**Theorem**: A policy ğœ‹(s, a) achieves the optimal value from state s (i.e. v<sub>ğœ‹</sub>(s) = v*(s)) if and only if for any state s' reachable from s, ğœ‹ achieves the optimal value from state s' (i.e. v<sub>ğœ‹</sub>(s') = v*(s')).

## Summary
| Problem | Tool | Algorithm |
|---------|------|-----------|
| Prediction | Bellman Expectation Equations | Iterative Policy Evaluation |
| Control | Bellman Expectation Equations + Greedy Policy Improvement | Policy Iteration |
| Control | Bellman Optimality Equation | Value Iteration |
 
