## Value Function (_V_, _q_):
A **policy** 𝜋 is a mapping from each state _s_ ∈ _S_ and action _a_ ∈ _A_(_s_) to the probability 𝜋(_s_, _a_) of taking action _a_ when in state _s_.

The **state-value function** is defined as the value of a state _s_ under a policy 𝜋. Mathematically, it is:\
_V_: _S_ ⭢ _𝓡_\
V<sub>𝜋</sub>(s) = E<sub>𝜋</sub> [R<sub>t+1</sub> + 𝛾R<sub>t+2</sub> + 𝛾<sup>2</sup>R<sub>t+2</sub>... | S<sub>t</sub> = s]

The **action-value function** is defined as the value of taking action _a_ in state _s_ under a policy 𝜋. Mathematically, it is:\
_q_: _S_ x _A_ ⭢ _𝓡_\
q<sub>𝜋</sub>(s, a) = E<sub>𝜋</sub> [R<sub>t+1</sub> + 𝛾R<sub>t+2</sub> + 𝛾<sup>2</sup>R<sub>t+2</sub>... | S<sub>t</sub> = s, A<sub>t</sub> = a]

| Variable | Meaning |
| -------- | ------- |
|_S_| Set of all states|
|_A_| Set of all actions|
|_𝓡_| A real number|

### Example of Optimal Value Function for a Maze
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/001.PNG)

### Example of Optimal Policy for a Maze
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/002.PNG)

**Model**: predicts what the environment will do next
- Transitions: _P_ predicts next state.
- Rewards: _R_ predicts the next action (immediate reward).

Given any state _s_ and action _a_, the probability of each possible next state _s'_ is:\
P<sup>a</sup><sub>ss'</sub> = Pr [s<sub>t+1</sub> = s' | S<sub>t</sub> = s, A<sub>t</sub> = a]

Given any current state _s_ and action _a_, the expected reward is:\
R<sup>a</sup><sub>s</sub> = E [r<sub>t</sub> | S<sub>t</sub> = s, A<sub>t</sub> = a]

Note: it is not necessary to have a model, but when we have one, then we can do some planning.

### Prediction vs Control
- Prediction: Evaluate a policy 𝜋 (compute V<sub>𝜋</sub> or q<sub>𝜋</sub>)
- Control: find the optimal policy 𝜋<sub>&ast;</sub> (compute V<sub>𝜋&ast;</sub> or q<sub>𝜋&ast;</sub>)

## Markov Decision Processes (MDPs)
Formally describes the environment for RL. Defined by the 5-tuple (_S_, _A_, _P_, _R_, 𝛾).
| Variable | Meaning |
| -------- | ------- |
|_S_| Set of all states|
|_A_| Set of all actions|
|_P_| Probability of each possible next state |
|_R_| Reward associated with each transition |
|𝛾| The discount factor. 𝛾 ∈ [0, 1], 𝛾 ⭢ 0 means "myopic" and 𝛾 ⭢ 1 means "farsighted" |

### Transition Graph Example with a Recycling Robot
At each such time the robot decides whether it should (1) actively search for a can, (2) remain stationary and wait for someone to bring it a can, or 
(3) go back to home base to recharge its battery. Suppose the environment works as follows. The best way to find cans is to actively search for them, 
but this runs down the robot's battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. 
In this case the robot must shut down and wait to be rescued (producing a low reward).

**States (energy levels)**:\
_S_ = {low, high}

**Actions**:\
_A_(low) = {search, wait, recharge}\
_A_(high) = {search, wait}

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/012.PNG)
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/013.PNG)

Note that policies are stationary, meaning they are time independent.

### Markov Chains
Similar to a directed graph with probability labels on top of the arrows. The sum of all probablilites leaving a node must add up to 1.

Below is an example of a Markov chain with the picking balls with replacement example, along with a probability matrix:
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/005.PNG)

## Return
The return _G_<sub>t</sub> is the total discounted reward from timestep _t_. The explicit formula is below:
![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/008.PNG)

From this, we can rewrite the state-value function _V_ as:\
V<sub>𝜋</sub>(s) = E<sub>𝜋</sub> [G<sub>t</sub> | S<sub>t</sub> = s]

We can also rewrite the action-value function _q_ as:\
q<sub>𝜋</sub>(s, a) = E<sub>𝜋</sub> [G<sub>t</sub> | S<sub>t</sub> = s, A<sub>t</sub> = a]

### Bellman Expectation Equation for V<sub>𝜋</sub>
The Bellman Expectation Equation for _V_<sub>𝜋</sub> expresses a relationship between the value of a state and the values of its successor states. 
The mathematical definition is below:

![](https://github.com/stinsan/CS-5033-Machine-Learning/blob/master/Screenshots/011.PNG)
